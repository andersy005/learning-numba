{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CUDA Execution Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA programming model exposes two primary abstractions:\n",
    "- A memory hierarchy\n",
    "- A thread hierarchy\n",
    "\n",
    "These abstractions allow us to control the massively parallel GPU. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Architecture Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The GPU architecture is built around a scalable array of **Streaming Multiprocessors (SM)**\n",
    "\n",
    "- Each SM in a GPU is designed to support concurrent execution of hundreds of threads, and there are generally multiple SMs per GPU\n",
    "  - So it is possible to have 1000s of threads executing concurrently on a single GPU.\n",
    "  \n",
    "- CUDA employs a **Single Instruction Multiple Thread (SIMT)** architecture to manage and execute threads. \n",
    "\n",
    "   - There can be as many as 32 CUDA threads (aka **warp**) in-flight running on same CUDA core.\n",
    "   \n",
    "- SIMT vs SIMD (Single Instruction Multiple Data)\n",
    "   - Both implement parallelism by broadcasting the same instruction to muliple execution units\n",
    "   - SIMD requires that all vector elements in a vector execute together in a unified synchronous group, whereas SIMT allows multiple threads in the same warp to execute indipendently. \n",
    "   - SIMT model includes three key features that SIMD does not\n",
    "     - Each thread has its own instruction address counter\n",
    "     - Each thread has its own register state\n",
    "     - Each thread can have an independent execution path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/fermi-sm-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key components of a Fermi SM: \n",
    "\n",
    "- ➤ CUDA Cores \n",
    "- ➤ Shared Memory/L1 Cache \n",
    "- ➤ Register File \n",
    "- ➤ Load/Store Units \n",
    "- ➤ Special Function Units \n",
    "- ➤ Warp Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A thread block is scheduled on only one SM.\n",
    "- Once a thread block is scheduled on an SM, it remains there until execution completes\n",
    "- An SM can hold more than one thread block at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/logical-view-and-haardware-view.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shared memory and registers are precious resources in an SM\n",
    "- Shared memory is partitioned among **thread blocks resident on the SM** and registers are partititioned among **threads**.\n",
    "- Threads in a thread block can cooperate and communicate with each other through these resources.\n",
    "- sharing data among parallel threads may cause a race condition\n",
    "  - CUDA provides a means to synchronize threads within a thread block to ensure all threads reach certain points in execution before making further progress.\n",
    "  - However, no primitives are provided for inter-block synchronization\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Nature of Warp Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wraps and Thread Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Warps are the basic unit of execution in an SM\n",
    "- When you launch a grid of thread blocks, the thread blocks in the grid are distributed among SMs.\n",
    "- Once a thread block is scheduled to an SM, threads in the thread block are further partitioned into warps\n",
    "- A warp consists of 32 consecutive threads\n",
    "   - All threads in a warp are executed in SMIPT fashion\n",
    "   - All threads execute the same instruction, and each thread carries out that operation on its own private data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/warp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thread blocks can be configured to be **1D, 2D, or 3D**\n",
    "- However, from the hardware perspective, all threads are arranged one-dimensionally\n",
    "- Each thread has a unique ID in a block\n",
    "- For a 1-D thread block, the unique thread ID is stored in the CUDA built-in variable `threadIdx.x`\n",
    "- Threads with consecutive values for `threadIdx.x` are grouped into warps\n",
    "- For example, 1-D thread block with 128 threads will be organized into 4 warps:\n",
    "    \n",
    "```\n",
    "Warp 0: thread 0, thread 1, thread 2, ... thread 31 \n",
    "Warp 1: thread 32, thread 33, thread 34, ... thread 63 \n",
    "Warp 2: thread 64, thread 65, thread 66, ... thread 95 \n",
    "Warp 3: thread 96, thread 97, thread 98, ... thread 127\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The logical layout of a 2-D or 3-D thread block can be converted into its one-dimensional physical layout by using the `x` dimension as the innermost dimension, the `y` dimension as the second dimension, and the `z` dimension as the outermost.\n",
    "\n",
    "- For example, given a 2-D thread block, a unique identifier for each thread in a block can be calculated using the built-in `threadIdx` and `blockDim` variables:\n",
    "\n",
    "    `threadIdx.y * blockDim.x  + threadIdx,x`\n",
    "    \n",
    "- The same calculation for a 3-D thread block is as follows:\n",
    "\n",
    "    `threadIdx.z * blockDim.y * blockDim.x + threadIdx.y * blockDim.x + threadIdx.x`\n",
    "    \n",
    "- The number of warps for a thread block can be determined as follows:\n",
    "\n",
    "$$\\text{WarpsPerBlock}=ceil(\\frac{\\text{ThreadsPerBlock}}{\\text{warpSize}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warp Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wrap divergence occurs when threads within a wrap take different code paths.\n",
    "- Different `if-then-else` branches are executed serially.\n",
    "- Try to adjust the granularity to be a multiple of warp size to avoid warp divergence.\n",
    "- Different warps can execute different code with no penalty on performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latency Hiding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THROUGHPUT AND BANDWIDTH**\n",
    "\n",
    "- Bandwidth and throughput are often confused, but may be used interchangeably depending on the situation. Both throughput and bandwidth are rate metrics used to measure performance.\n",
    "- Bandwidth is usually used to refer to a theoretical peak value, while throughput is used to refer to an achieved value.\n",
    "- Bandwidth is usually used to describe the highest possible amount of data transfer per time unit, while throughput can be used to describe the rate of any kind of information or operations carried out per time unit, such as, how many instruc- tions are completed per cycle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPOSING SUFFICIENT PARALLELISM**\n",
    "\n",
    "- Because the GPU partitions compute resources among threads, switching between concurrent warps has very little overhead (on the order of one or two cycles) as the required state is already available on-chip. If there are sufficient concurrently active threads, you can keep the GPU busy in every pipeline stage on every cycle. In this situation, the latency of one warp is hidden by the execution of other warps. Therefore, exposing sufficient parallelism to SMs is beneficial to performance.\n",
    "- A simple formula for calculating the required parallelism is to multiply the number of cores per SM by the latency of one arithmetic instruction on that SM. For example, Fermi has 32 single-precision floating-point pipeline lanes and the latency of one arithmetic instruction is 20 cycles, so at minimum 32 x 20 = 640 threads per SM are required to keep your device busy. However, this is a lower bound.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occupancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GUIDELINES FOR GRID AND BLOCK SIZE**\n",
    "Using these guidelines will help your application scale on current and future devices: \n",
    "- ➤ Keep the number of threads per block a multiple of warp size (32). \n",
    "- ➤ Avoid small block sizes: Start with at least 128 or 256 threads per block. \n",
    "- ➤ Adjust block size up or down according to kernel resource requirements.\n",
    "- ➤ Keep the number of blocks much greater than the number of SMs to expose suffi cient parallelism to your device.\n",
    "- ➤ Conduct experiments to discover the best execution confi guration and resource usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CUDA, barrier synchronization can be performed at two levels:\n",
    "\n",
    "- **System-level:** Wait for all work on both the host and the device to complete\n",
    "- **Block-level:** Wait for all threads in a thread block to reach the same point in execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
